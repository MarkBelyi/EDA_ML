{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #ffffcc; padding: 10px; border-radius: 5px;\">\n",
    "The aim of this assignment is to train a simple model to predict the duration of a ride, similar to what we practiced during the lecture.\n",
    "</p>\n",
    "\n",
    "\n",
    "<h3>Q1. Data Download</h3>\n",
    "\n",
    "<p>We'll be using the same <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\">NYC taxi dataset</a>, but instead of the <strong>Green Taxi Trip Records</strong>, we'll work with the <strong>Yellow Taxi Trip Records</strong>.</p>\n",
    "\n",
    "<p>Download the data for January and February 2023.</p>\n",
    "\n",
    "<p>Load the January data. How many columns are present?</p>\n",
    "\n",
    "<ul>\n",
    "    <li>16</li>\n",
    "    <li>17</li>\n",
    "    <li>18</li>\n",
    "    <li>19</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Q2. Calculating Duration</h3>\n",
    "\n",
    "<p>Now, let's calculate the <code>duration</code> variable. This variable should represent the duration of a ride in minutes.</p>\n",
    "\n",
    "<p>What is the standard deviation of the trip durations for January?</p>\n",
    "\n",
    "<ul>\n",
    "    <li>32.59</li>\n",
    "    <li>42.59</li>\n",
    "    <li>52.59</li>\n",
    "    <li>62.59</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:39.740514Z",
     "start_time": "2024-11-03T07:49:39.478943200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "19"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "df_january = pd.read_parquet('yellow_tripdata_2023-01.parquet')\n",
    "df_january.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:40.884979700Z",
     "start_time": "2024-11-03T07:49:39.509878900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n0               2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n1               2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n2               2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n3               1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n4               2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n...           ...                  ...                   ...              ...   \n3066761         2  2023-01-31 23:58:34   2023-02-01 00:12:33              NaN   \n3066762         2  2023-01-31 23:31:09   2023-01-31 23:50:36              NaN   \n3066763         2  2023-01-31 23:01:05   2023-01-31 23:25:36              NaN   \n3066764         2  2023-01-31 23:40:00   2023-01-31 23:53:00              NaN   \n3066765         2  2023-01-31 23:07:32   2023-01-31 23:21:56              NaN   \n\n         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n0                 0.97         1.0                  N           161   \n1                 1.10         1.0                  N            43   \n2                 2.51         1.0                  N            48   \n3                 1.90         1.0                  N           138   \n4                 1.43         1.0                  N           107   \n...                ...         ...                ...           ...   \n3066761           3.05         NaN               None           107   \n3066762           5.80         NaN               None           112   \n3066763           4.67         NaN               None           114   \n3066764           3.15         NaN               None           230   \n3066765           2.85         NaN               None           262   \n\n         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n0                 141             2         9.30   1.00      0.5        0.00   \n1                 237             1         7.90   1.00      0.5        4.00   \n2                 238             1        14.90   1.00      0.5       15.00   \n3                   7             1        12.10   7.25      0.5        0.00   \n4                  79             1        11.40   1.00      0.5        3.28   \n...               ...           ...          ...    ...      ...         ...   \n3066761            48             0        15.80   0.00      0.5        3.96   \n3066762            75             0        22.43   0.00      0.5        2.64   \n3066763           239             0        17.61   0.00      0.5        5.32   \n3066764            79             0        18.15   0.00      0.5        4.43   \n3066765           143             0        15.97   0.00      0.5        2.00   \n\n         tolls_amount  improvement_surcharge  total_amount  \\\n0                 0.0                    1.0         14.30   \n1                 0.0                    1.0         16.90   \n2                 0.0                    1.0         34.90   \n3                 0.0                    1.0         20.85   \n4                 0.0                    1.0         19.68   \n...               ...                    ...           ...   \n3066761           0.0                    1.0         23.76   \n3066762           0.0                    1.0         29.07   \n3066763           0.0                    1.0         26.93   \n3066764           0.0                    1.0         26.58   \n3066765           0.0                    1.0         21.97   \n\n         congestion_surcharge  airport_fee  \n0                         2.5         0.00  \n1                         2.5         0.00  \n2                         2.5         0.00  \n3                         0.0         1.25  \n4                         2.5         0.00  \n...                       ...          ...  \n3066761                   NaN          NaN  \n3066762                   NaN          NaN  \n3066763                   NaN          NaN  \n3066764                   NaN          NaN  \n3066765                   NaN          NaN  \n\n[3066766 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VendorID</th>\n      <th>tpep_pickup_datetime</th>\n      <th>tpep_dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_distance</th>\n      <th>RatecodeID</th>\n      <th>store_and_fwd_flag</th>\n      <th>PULocationID</th>\n      <th>DOLocationID</th>\n      <th>payment_type</th>\n      <th>fare_amount</th>\n      <th>extra</th>\n      <th>mta_tax</th>\n      <th>tip_amount</th>\n      <th>tolls_amount</th>\n      <th>improvement_surcharge</th>\n      <th>total_amount</th>\n      <th>congestion_surcharge</th>\n      <th>airport_fee</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2023-01-01 00:32:10</td>\n      <td>2023-01-01 00:40:36</td>\n      <td>1.0</td>\n      <td>0.97</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>161</td>\n      <td>141</td>\n      <td>2</td>\n      <td>9.30</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>14.30</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2023-01-01 00:55:08</td>\n      <td>2023-01-01 01:01:27</td>\n      <td>1.0</td>\n      <td>1.10</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>43</td>\n      <td>237</td>\n      <td>1</td>\n      <td>7.90</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>4.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>16.90</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2023-01-01 00:25:04</td>\n      <td>2023-01-01 00:37:49</td>\n      <td>1.0</td>\n      <td>2.51</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>48</td>\n      <td>238</td>\n      <td>1</td>\n      <td>14.90</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>15.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>34.90</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2023-01-01 00:03:48</td>\n      <td>2023-01-01 00:13:25</td>\n      <td>0.0</td>\n      <td>1.90</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>138</td>\n      <td>7</td>\n      <td>1</td>\n      <td>12.10</td>\n      <td>7.25</td>\n      <td>0.5</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>20.85</td>\n      <td>0.0</td>\n      <td>1.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>2023-01-01 00:10:29</td>\n      <td>2023-01-01 00:21:19</td>\n      <td>1.0</td>\n      <td>1.43</td>\n      <td>1.0</td>\n      <td>N</td>\n      <td>107</td>\n      <td>79</td>\n      <td>1</td>\n      <td>11.40</td>\n      <td>1.00</td>\n      <td>0.5</td>\n      <td>3.28</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>19.68</td>\n      <td>2.5</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3066761</th>\n      <td>2</td>\n      <td>2023-01-31 23:58:34</td>\n      <td>2023-02-01 00:12:33</td>\n      <td>NaN</td>\n      <td>3.05</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>107</td>\n      <td>48</td>\n      <td>0</td>\n      <td>15.80</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>3.96</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>23.76</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3066762</th>\n      <td>2</td>\n      <td>2023-01-31 23:31:09</td>\n      <td>2023-01-31 23:50:36</td>\n      <td>NaN</td>\n      <td>5.80</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>112</td>\n      <td>75</td>\n      <td>0</td>\n      <td>22.43</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>2.64</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>29.07</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3066763</th>\n      <td>2</td>\n      <td>2023-01-31 23:01:05</td>\n      <td>2023-01-31 23:25:36</td>\n      <td>NaN</td>\n      <td>4.67</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>114</td>\n      <td>239</td>\n      <td>0</td>\n      <td>17.61</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>5.32</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>26.93</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3066764</th>\n      <td>2</td>\n      <td>2023-01-31 23:40:00</td>\n      <td>2023-01-31 23:53:00</td>\n      <td>NaN</td>\n      <td>3.15</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>230</td>\n      <td>79</td>\n      <td>0</td>\n      <td>18.15</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>4.43</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>26.58</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3066765</th>\n      <td>2</td>\n      <td>2023-01-31 23:07:32</td>\n      <td>2023-01-31 23:21:56</td>\n      <td>NaN</td>\n      <td>2.85</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>262</td>\n      <td>143</td>\n      <td>0</td>\n      <td>15.97</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>2.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>21.97</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3066766 rows Ã— 19 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "df_january"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:40.943356400Z",
     "start_time": "2024-11-03T07:49:40.884979700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "42.59"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_january['duration'] = (pd.to_datetime(df_january['tpep_dropoff_datetime']) - pd.to_datetime(df_january['tpep_pickup_datetime'])).dt.total_seconds() / 60 # so it minutes now\n",
    "std_duration = df_january['duration'].std()\n",
    "round(std_duration, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:41.684639500Z",
     "start_time": "2024-11-03T07:49:40.924494200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q3. Removing Outliers</h3>\n",
    "\n",
    "<p>Next, let's examine the distribution of the <code>duration</code> variable. There are some outliers present, so we'll remove them and keep only the records where the duration is between 1 and 60 minutes (inclusive).</p>\n",
    "\n",
    "<p>What fraction of records remains after removing the outliers?</p>\n",
    "\n",
    "<ul>\n",
    "    <li>90%</li>\n",
    "    <li>92%</li>\n",
    "    <li>95%</li>\n",
    "    <li>98%</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Q4. One-Hot Encoding</h3>\n",
    "\n",
    "<p>Now, let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Convert the dataframe to a list of dictionaries (remember to recast the IDs as strings to avoid label encoding).</li>\n",
    "    <li>Apply a dictionary vectorizer.</li>\n",
    "    <li>Generate a feature matrix from it.</li>\n",
    "</ul>\n",
    "\n",
    "<p>What is the dimensionality of this matrix (i.e., the number of columns)?</p>\n",
    "\n",
    "<ul>\n",
    "    <li>2</li>\n",
    "    <li>155</li>\n",
    "    <li>345</li>\n",
    "    <li>515</li>\n",
    "    <li>715</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "#Q3\n",
    "df_filtered = df_january[(df_january['duration'] >= 1) &\n",
    "(df_january['duration'] <= 60)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:42.444162700Z",
     "start_time": "2024-11-03T07:49:41.684021500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "98.0"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_remaining = len(df_filtered) / len(df_january)\n",
    "round(fraction_remaining * 100, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:49:42.456346500Z",
     "start_time": "2024-11-03T07:49:42.444693300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santa\\AppData\\Local\\Temp\\ipykernel_3832\\3276235443.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['PULocationID'] = df_filtered['PULocationID'].astype(str)\n",
      "C:\\Users\\santa\\AppData\\Local\\Temp\\ipykernel_3832\\3276235443.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['DOLocationID'] = df_filtered['DOLocationID'].astype(str)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.5 GiB for an array with shape (3009173, 515) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m data_dicts \u001B[38;5;241m=\u001B[39m df_filtered[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPULocationID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDOLocationID\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mto_dict(orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m dv \u001B[38;5;241m=\u001B[39m DictVectorizer(sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 8\u001B[0m feature_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mdv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dicts\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\EploratoryDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 316\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    318\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    319\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    320\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    321\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    322\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\EploratoryDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\EploratoryDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:316\u001B[0m, in \u001B[0;36mDictVectorizer.fit_transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    294\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001B[39;00m\n\u001B[0;32m    295\u001B[0m \n\u001B[0;32m    296\u001B[0m \u001B[38;5;124;03m    Like fit(X) followed by transform(X), but does not require\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        Feature vectors; always 2-d.\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfitting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\EploratoryDataAnalysis\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:284\u001B[0m, in \u001B[0;36mDictVectorizer._transform\u001B[1;34m(self, X, fitting)\u001B[0m\n\u001B[0;32m    282\u001B[0m     result_matrix\u001B[38;5;241m.\u001B[39msort_indices()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 284\u001B[0m     result_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mresult_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fitting:\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_names_ \u001B[38;5;241m=\u001B[39m feature_names\n",
      "File \u001B[1;32m~\\PycharmProjects\\EploratoryDataAnalysis\\venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1181\u001B[0m, in \u001B[0;36m_cs_matrix.toarray\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m order \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1180\u001B[0m     order \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_swap(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcf\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m-> 1181\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_toarray_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mc_contiguous \u001B[38;5;129;01mor\u001B[39;00m out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mf_contiguous):\n\u001B[0;32m   1183\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput array must be C or F contiguous\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\EploratoryDataAnalysis\\venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1301\u001B[0m, in \u001B[0;36m_spbase._process_toarray_args\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m   1300\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 11.5 GiB for an array with shape (3009173, 515) and data type float64"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "df_filtered['PULocationID'] = df_filtered['PULocationID'].astype(str)\n",
    "df_filtered['DOLocationID'] = df_filtered['DOLocationID'].astype(str)\n",
    "\n",
    "data_dicts = df_filtered[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "feature_matrix = dv.fit_transform(data_dicts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T07:50:08.229383200Z",
     "start_time": "2024-11-03T07:49:42.452640200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_matrix.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q5. Model Training</h3>\n",
    "\n",
    "<p>Let's now use the feature matrix from the previous step to train a model.</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Train a simple linear regression model with default parameters, with <code>duration</code> as the target variable.</li>\n",
    "    <li>Calculate the RMSE of the model on the training dataset.</li>\n",
    "</ul>\n",
    "\n",
    "<p>What is the RMSE on the training data?</p>\n",
    "\n",
    "<ul>\n",
    "    <li>3.64</li>\n",
    "    <li>7.64</li>\n",
    "    <li>11.64</li>\n",
    "    <li>16.64</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Q6. Model Evaluation</h3>\n",
    "\n",
    "<p>Next, let's evaluate this model using the validation dataset (February 2023).</p>\n",
    "\n",
    "<p>What is the RMSE on the validation data?</p>\n",
    "\n",
    "<ul>\n",
    "    <li>3.81</li>\n",
    "    <li>7.81</li>\n",
    "    <li>11.81</li>\n",
    "    <li>16.81</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Q5\n",
    "y = df_filtered['duration']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(feature_matrix, y)\n",
    "\n",
    "y_pred = model.predict(feature_matrix)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "round(rmse, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Q6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_february = pd.read_parquet('yellow_tripdata_2023-02.parquet')\n",
    "\n",
    "df_february['duration'] = (pd.to_datetime(df_february['tpep_dropoff_datetime']) - pd.to_datetime(df_february['tpep_pickup_datetime'])).dt.total_seconds() / 60\n",
    "\n",
    "\n",
    "df_february_filtered = df_february[(df_february['duration'] >= 1) & (df_february['duration'] <= 60)]\n",
    "\n",
    "df_february_filtered['PULocationID'] = df_february_filtered['PULocationID'].astype(str)\n",
    "df_february_filtered['DOLocationID'] = df_february_filtered['DOLocationID'].astype(str)\n",
    "\n",
    "data_dicts_val = df_february_filtered[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "\n",
    "feature_matrix_val = dv.transform(data_dicts_val)\n",
    "\n",
    "y_val = df_february_filtered['duration']\n",
    "\n",
    "y_val_pred = model.predict(feature_matrix_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
